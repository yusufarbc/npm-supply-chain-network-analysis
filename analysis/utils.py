import os
import pandas as pd
import networkx as nx

def export_results(risk_df, G, impact_df=None, output_dir='../results'):
    # Ensure results directory exists (at project root)
    os.makedirs(output_dir, exist_ok=True)

    print("Exporting results...")

    # 1. Full Analysis Table
    risk_df.to_csv(f'{output_dir}/risk_scores.csv', index=False)
    print(f" - Saved {output_dir}/risk_scores.csv")

    # 1b. Simplified BRS List (Package, Risk Score)
    risk_df[['package', 'risk_score']].to_csv(f'{output_dir}/package_risk_scores.csv', index=False)
    print(f" - Saved {output_dir}/package_risk_scores.csv")

    # 1c. Cascade Impact Scores (if available)
    if impact_df is not None and not impact_df.empty:
        impact_df.to_csv(f'{output_dir}/impact_scores.csv', index=False)
        print(f" - Saved {output_dir}/impact_scores.csv")

    # 2. Gephi Nodes
    # Create numeric ID map for Gephi (safer than string IDs)
    # Sort by rank/risk for consistent IDs
    sorted_nodes = risk_df.sort_values('risk_score', ascending=False)['package'].tolist()
    id_map = {pkg: i+1 for i, pkg in enumerate(sorted_nodes)}
    
    # Prepare Gephi DataFrame
    gephi_nodes = risk_df.copy()
    gephi_nodes['Id'] = gephi_nodes['package'].map(id_map)
    gephi_nodes['Label'] = gephi_nodes['package']
    
    # Rename columns to Gephi standard (CamelCase often preferred)
    col_mapping = {
        'risk_score': 'RiskScore',
        'in_degree': 'InDegree',
        'out_degree': 'OutDegree',
        'betweenness': 'Betweenness',
        'dependents_count': 'DependentsCount',
        'downloads': 'Downloads',
        'rank': 'Rank',
        'type': 'Type'
    }
    gephi_nodes = gephi_nodes.rename(columns=col_mapping)
    
    # Select columns that exist
    cols_to_export = ['Id', 'Label'] + [c for c in col_mapping.values() if c in gephi_nodes.columns]
    gephi_nodes = gephi_nodes[cols_to_export]
    
    gephi_nodes.to_csv(f'{output_dir}/gephi_nodes.csv', index=False)
    print(f" - Saved {output_dir}/gephi_nodes.csv")

    # 3. Gephi Edges
    # Columns: Source, Target, Type
    gephi_edges = pd.DataFrame(list(G.edges()), columns=['Source', 'Target'])
    
    # Map Source and Target to numeric IDs
    gephi_edges['Source'] = gephi_edges['Source'].map(id_map)
    gephi_edges['Target'] = gephi_edges['Target'].map(id_map)
    
    # Filter out edges where nodes might be missing (shouldn't happen but safe)
    gephi_edges = gephi_edges.dropna()
    gephi_edges['Source'] = gephi_edges['Source'].astype(int)
    gephi_edges['Target'] = gephi_edges['Target'].astype(int)
    
    gephi_edges['Type'] = 'Directed'
    gephi_edges.to_csv(f'{output_dir}/gephi_edges.csv', index=False)
    print(f" - Saved {output_dir}/gephi_edges.csv")

    # 4. Generate README.md Report
    generate_results_readme(risk_df, G, impact_df, output_dir)

def generate_results_readme(risk_df, G, impact_df, output_dir):
    readme_path = f'{output_dir}/Readme.md'
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write("# Analysis Results\n\n")
        f.write("Automated analysis report generated by the pipeline.\n\n")
        
        # Network Statistics
        f.write("## üìä Network Statistics\n")
        f.write(f"- **Nodes**: {G.number_of_nodes()}\n")
        f.write(f"- **Edges**: {G.number_of_edges()}\n")
        f.write(f"- **Density**: {nx.density(G):.6f}\n")
        if nx.is_directed(G):
            f.write(f"- **Strongly Connected Components**: {nx.number_strongly_connected_components(G)}\n")
            f.write(f"- **Weakly Connected Components**: {nx.number_weakly_connected_components(G)}\n")
        f.write("\n")

        # Top 20 Risk Score
        f.write("## üö® Top 20 Critical Packages (Risk Score)\n")
        f.write("Packages with the highest Composite Risk Score (BRS).\n\n")
        top_risk = risk_df.sort_values('risk_score', ascending=False).head(20)
        # Manual markdown table generation to avoid tabulate dependency if not present
        f.write("| Package | Risk Score | In-Degree | Betweenness |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for _, row in top_risk.iterrows():
            f.write(f"| {row['package']} | {row['risk_score']:.4f} | {row['in_degree']:.4f} | {row['betweenness']:.4f} |\n")
        f.write("\n")
        
        # Top 20 In-Degree
        f.write("## üîó Top 20 Most Depended-upon Packages (In-Degree)\n")
        f.write("Packages that have the most direct dependents in this network.\n\n")
        top_in = risk_df.sort_values('in_degree', ascending=False).head(20)
        f.write("| Package | In-Degree | Out-Degree | Risk Score |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for _, row in top_in.iterrows():
            f.write(f"| {row['package']} | {row['in_degree']:.4f} | {row['out_degree']:.4f} | {row['risk_score']:.4f} |\n")
        f.write("\n")

        # Top 20 Out-Degree
        f.write("## üì¶ Top 20 Most Dependent Packages (Out-Degree)\n")
        f.write("Packages that have the most dependencies (highest complexity).\n\n")
        top_out = risk_df.sort_values('out_degree', ascending=False).head(20)
        f.write("| Package | Out-Degree | In-Degree | Risk Score |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for _, row in top_out.iterrows():
            f.write(f"| {row['package']} | {row['out_degree']:.4f} | {row['in_degree']:.4f} | {row['risk_score']:.4f} |\n")
        f.write("\n")

        # Top 20 Betweenness
        f.write("## üåâ Top 20 Bridges (Betweenness Centrality)\n")
        f.write("Packages that act as bridges or bottlenecks in the network.\n\n")
        top_bet = risk_df.sort_values('betweenness', ascending=False).head(20)
        f.write("| Package | Betweenness | In-Degree | Risk Score |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for _, row in top_bet.iterrows():
            f.write(f"| {row['package']} | {row['betweenness']:.4f} | {row['in_degree']:.4f} | {row['risk_score']:.4f} |\n")
        f.write("\n")

        # Top 20 Dependents Count
        f.write("## üåç Top 20 Ecosystem Critical (Dependents Count)\n")
        f.write("Packages with the highest number of dependents in the entire NPM ecosystem.\n\n")
        top_dep = risk_df.sort_values('dependents_count', ascending=False).head(20)
        f.write("| Package | Dependents Count | In-Degree | Risk Score |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for _, row in top_dep.iterrows():
            f.write(f"| {row['package']} | {row['dependents_count']:,} | {row['in_degree']:.4f} | {row['risk_score']:.4f} |\n")
        f.write("\n")

        # Top 20 Cascade Impact (if available)
        if impact_df is not None:
            f.write("## üí• Top 20 Cascade Impact (Network Damage)\n")
            f.write("Packages that cause the largest reduction in network connectivity (LCC) when removed.\n\n")
            top_impact = impact_df.sort_values('cascade_impact', ascending=False).head(20)
            f.write("| Package | Cascade Impact | Risk Score |\n")
            f.write("| :--- | :--- | :--- |\n")
            # Merge with risk_df to get risk score if not present
            if 'risk_score' not in top_impact.columns:
                top_impact = top_impact.merge(risk_df[['package', 'risk_score']], on='package', how='left')
            
            for _, row in top_impact.iterrows():
                f.write(f"| {row['package']} | {row['cascade_impact']:.4f} | {row.get('risk_score', 0):.4f} |\n")
            f.write("\n")
        
    print(f" - Saved {readme_path}")
